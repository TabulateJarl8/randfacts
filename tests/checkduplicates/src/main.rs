//! A test for finding and managing duplicate facts across files.
//! Disclaimer: comments mostly generated by AI

use clap::{command, Arg, ArgAction};
use indicatif::{ParallelProgressIterator, ProgressBar, ProgressStyle};
use rayon::iter::ParallelIterator;
use rayon::prelude::*;
use structures::{DuplicateFactMatch, FactClass, SIMILARITY_THRESHOLD};

mod structures;
mod util;

/// Calculates the similarity ratio between two strings using a token sort approach.
///
/// This function implements a modified version of token sort ratio that:
/// 1. Performs early exit optimization for strings with significantly different lengths
/// 2. Filters out non-alphanumeric characters
/// 3. Converts all characters to lowercase for comparison
///
/// # Arguments
/// * `str1` - First string to compare
/// * `str2` - Second string to compare
///
/// # Returns
/// A float between 0 and 100 representing the similarity percentage,
/// with a -5 offset to reduce false positives
#[inline(always)]
fn token_sort_ratio(str1: &str, str2: &str) -> f64 {
    let len1 = str1.len();
    let len2 = str2.len();

    // Early exit for obviously different strings
    // if their lengths differ by more than half, they're most likely different enough
    // this may lead to issues, but it lead to a ~23.33% performance improvement
    if (len1 as f64 / len2 as f64) < 0.5 || (len2 as f64 / len1 as f64) < 0.5 {
        return 0.0;
    }

    // Preallocate vectors with capacity
    let mut vec1 = Vec::with_capacity(len1);
    let mut vec2 = Vec::with_capacity(len2);

    // Filter and collect bytes in one pass
    vec1.extend(
        str1.bytes()
            .filter(|&b| b.is_ascii_alphanumeric())
            .map(|b| b.to_ascii_lowercase()),
    );

    vec2.extend(
        str2.bytes()
            .filter(|&b| b.is_ascii_alphanumeric())
            .map(|b| b.to_ascii_lowercase()),
    );

    // Calculate wagner fischer directly on character vectors
    let dist = wagner_fischer_2row(&vec1, &vec2) as f64;
    let maximum = vec1.len() + vec2.len();

    if maximum == 0 {
        return 0.0;
    }

    // Convert distance to similarity ratio and subtract 5 to reduce false positives
    (1.0 - (dist / maximum as f64)) * 100.0 - 5.0
}

/// Implements the Wagner-Fischer algorithm for calculating edit distance between two sequences,
/// optimized to use only two rows of memory.
///
/// # Arguments
/// * `s1` - First sequence of characters
/// * `s2` - Second sequence of characters
///
/// # Returns
/// The minimum number of single-character edits needed to transform one string into another
#[inline(always)]
fn wagner_fischer_2row(s1: &[u8], s2: &[u8]) -> usize {
    // Ensure s1 is the shorter sequence for optimization
    let (s1, s2) = if s1.len() < s2.len() {
        (s1, s2)
    } else {
        (s2, s1)
    };

    let len1 = s1.len();
    let len2 = s2.len();

    // handle empty string cases
    if len1 == 0 {
        return len2;
    }
    if len2 == 0 {
        return len1;
    }

    // Initialize two rows for the dynamic programming matrix
    let mut prev_row = vec![0; len2 + 1];
    let mut curr_row = vec![0; len2 + 1];

    // Initialize first row with incremental values
    (0..=len2).for_each(|i| {
        prev_row[i] = i;
    });

    // Fill the matrix using only two rows
    for (i, c1) in s1.iter().enumerate() {
        curr_row[0] = i + 1;

        for (j, c2) in s2.iter().enumerate() {
            curr_row[j + 1] = if c1 == c2 {
                // No edit needed
                prev_row[j]
            } else {
                // Take minimum of three possible operations (insert, delete, substitute)
                1 + prev_row[j].min(prev_row[j + 1]).min(curr_row[j])
            };
        }

        // Swap rows using mem::swap for better performance
        std::mem::swap(&mut prev_row, &mut curr_row);
    }

    prev_row[len2]
}

/// Finds duplicate facts across safe and unsafe fact files using parallel processing.
///
/// This function:
/// 1. Loads facts from both safe.txt and unsafe.txt
/// 2. Generates all possible pairs of facts
/// 3. Calculates similarity ratios in parallel
/// 4. Returns matches above the similarity threshold
///
/// # Returns
/// A vector of DuplicateFactMatch containing similar fact pairs and their similarity scores
fn find_duplicate_facts() -> Vec<DuplicateFactMatch> {
    // read safe.txt and unsafe.txt into lists
    let mut all_facts = util::load_fact_list("safe.txt", FactClass::Safe);

    let mut unsafe_contents = util::load_fact_list("unsafe.txt", FactClass::Unsafe);

    all_facts.append(&mut unsafe_contents);

    // Calculate total number of possible combinations for progress bar
    let total_facts = all_facts.len() as u64;
    let total_combinations = num_integer::binomial(total_facts as u64, 2);

    // Initialize progress bar with custom style
    let pb = ProgressBar::new(total_combinations);
    pb.set_style(
        ProgressStyle::default_bar()
            .template(
                "{percent}% |{wide_bar}| {pos}/{len} [{elapsed_precise}<{eta_precise} {per_sec}]",
            )
            .unwrap(),
    );

    // Generate all possible indices combinations
    let indices: Vec<_> = (0..all_facts.len())
        .flat_map(|i| ((i + 1)..all_facts.len()).map(move |j| (i, j)))
        .collect();

    // Process combinations in parallel
    indices
        .into_par_iter()
        .progress_with(pb)
        .filter_map(|(i, j)| {
            let facts = &all_facts;
            let fact1 = &facts[i];
            let fact2 = &facts[j];

            let ratio = token_sort_ratio(&fact1.fact, &fact2.fact);
            if ratio > SIMILARITY_THRESHOLD {
                Some((fact1.clone(), fact2.clone(), ratio))
            } else {
                None
            }
        })
        .collect()
}

fn main() {
    let args = command!()
        .arg(
            Arg::new("fix_duplicates")
                .long("fix-duplicates")
                .action(ArgAction::SetTrue)
                .help("Remove duplicate facts"),
        )
        .get_matches();

    let matches = find_duplicate_facts();

    if !matches.is_empty() {
        if !args.get_flag("fix_duplicates") {
            println!("{:#?}", matches);
            println!("\nNumber of similar facts: {}", matches.len());
            std::process::exit(1);
        }

        // Fix mode: Remove duplicates
        println!("Generating list of indicies to remove...");
        let mut indicies_to_remove = vec![];

        // Determine which facts to remove, prioritizing keeping unsafe facts
        for fact_match in matches {
            println!("{:#?}", fact_match);

            // keep unsafe facts over safe facts
            if fact_match.0.class == FactClass::Unsafe {
                indicies_to_remove.push((fact_match.0.line_number, fact_match.0.class));
            } else {
                // first fact isn't unsafe so we don't need to prioritize it
                indicies_to_remove.push((fact_match.1.line_number, fact_match.1.class));
            }
        }

        // Load current facts
        let mut safe_facts = util::load_fact_list("safe.txt", FactClass::Safe);
        let mut unsafe_facts = util::load_fact_list("unsafe.txt", FactClass::Unsafe);

        // sort removal indicies in reverse to maintain correct line numbers
        indicies_to_remove.sort_unstable_by(|a, b| b.0.partial_cmp(&a.0).unwrap());

        // Remove duplicates from respective files
        for (index, class) in indicies_to_remove {
            match class {
                FactClass::Safe => safe_facts.remove(index),
                FactClass::Unsafe => unsafe_facts.remove(index),
            };
        }

        // Write updated facts back to files
        util::write_facts_to_file("safe.txt", &safe_facts);
        util::write_facts_to_file("unsafe.txt", &unsafe_facts);
    }
}
